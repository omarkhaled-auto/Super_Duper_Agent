# v13: Research-to-Testing Pipeline — Exhaustive Implementation Prompt

> **Version:** 13.0
> **Scope:** Connect the research fleet's domain knowledge to E2E and browser testing agents via a generated TESTING_GUIDANCE.md artifact.
> **Estimated size:** ~200 new lines across 4 files, ~150 test lines, 0 new files (all edits to existing modules).

---

## THE PROBLEM (with concrete failure evidence)

The research fleet writes domain-specific findings to REQUIREMENTS.md. Code-writers consume them. But the 6 testing prompts (`BACKEND_E2E_PROMPT`, `FRONTEND_E2E_PROMPT`, `E2E_FIX_PROMPT`, `BROWSER_WORKFLOW_EXECUTOR_PROMPT`, `BROWSER_WORKFLOW_FIX_PROMPT`, `BROWSER_REGRESSION_SWEEP_PROMPT`) start fresh with hardcoded instructions and never read research findings. This causes three categories of preventable test failure:

**Failure 1 — Component library selector blindness:** The research fleet uses Context7 to look up PrimeNG docs and learns that `<p-table>` renders as `<div class="p-datatable">` with `role="grid"`. The code-writer uses this knowledge to build correct components. But the E2E agent writes `page.getByTestId('project-table')` because its prompt only says "use `getByRole`, `getByText`, `getByTestId`" — it doesn't know PrimeNG emits `role="grid"` on DataTable. The test fails, burns 5 fix retries, and the fix agent still can't figure out the selector because it also never read the research findings.

**Failure 2 — Canvas interaction impossibility:** The research fleet learns from Fabric.js docs that canvas objects are not DOM elements — you can't click them via Playwright selectors. The correct approach is programmatic API calls (`canvas.getObjects()`, `canvas.setActiveObject()`) or coordinate-based mouse events. But the browser executor agent calls `browser_snapshot()`, sees a single `<canvas>` element with no children, tries to click inside it, fails, and reports "element not found." All retry budget wasted on an impossible approach.

**Failure 3 — Framework timing misunderstanding:** The research fleet learns Angular's change detection and `Zone.js` patterns — after navigation, data loads asynchronously and the DOM updates after a digest cycle. The E2E script-writer generates `await page.goto('/projects')` followed immediately by `expect(page.getByRole('grid')).toBeVisible()` without waiting for the HTTP response. The test flakes at 40% rate, passes sometimes, fails sometimes. The fix agent adds a `waitForTimeout(2000)` which is fragile. The research already documented that `page.waitForResponse('**/api/projects')` is the correct pattern.

**Root cause:** There is no artifact that transfers research findings to testing agents.

---

## THE FIX

### New artifact: `TESTING_GUIDANCE.md`

Generated by the orchestrator AFTER the research fleet completes (between steps 3 and 3.5 in the standard workflow, or between steps a and b in milestone execution). Contains:

1. **Component library testing patterns** — DOM structure, ARIA roles, recommended selectors
2. **Canvas/WebGL testing strategy** — what's possible and what's not, alternative approaches
3. **Framework timing patterns** — how to wait correctly for this specific framework
4. **Authentication flow specifics** — exact login sequence for this app's auth implementation
5. **Known gotchas** — library-specific quirks discovered during research

### Injection points (6 prompts)

Each testing prompt gets one new instruction block telling the agent to read TESTING_GUIDANCE.md before writing or executing tests.

---

## AGENT TEAM

| Agent | Role | Deliverable |
|-------|------|-------------|
| **researcher** | Read all 4 source files, map exact edit locations | Line-number-precise edit plan |
| **implementer-prompts** | Edit prompt constants in agents.py, e2e_testing.py, browser_testing.py | Modified prompt text |
| **implementer-orchestrator** | Edit orchestrator workflow in agents.py + milestone workflow | New step 3.25 / step a2 |
| **test-engineer** | Write tests for all changes | test_v13_research_testing_pipeline.py |
| **wiring-verifier** | Verify end-to-end flow works | Wiring verification report |

---

## COORDINATION WAVES

```
Wave 1: researcher (READ-ONLY — explores codebase, produces edit plan)
Wave 2: implementer-prompts + implementer-orchestrator (PARALLEL — independent edits)
Wave 3: test-engineer (writes tests against implemented changes)
Wave 4: wiring-verifier (verifies full pipeline integration)
```

---

## PHASE 1: DISCOVERY (researcher agent)

### 1.1 Read these files completely

| File | Path | What to find |
|------|------|-------------|
| `agents.py` | `src/agent_team/agents.py` | RESEARCHER_PROMPT (~line 774), orchestrator workflow steps 3-3.5 (~lines 562-564), milestone workflow step a (~line 351), Research Fleet deployment instructions (~lines 452-465) |
| `e2e_testing.py` | `src/agent_team/e2e_testing.py` | BACKEND_E2E_PROMPT (~line 556), FRONTEND_E2E_PROMPT (~line 732), E2E_FIX_PROMPT (~line 884). Each prompt's first instruction line that reads REQUIREMENTS.md |
| `browser_testing.py` | `src/agent_team/browser_testing.py` | BROWSER_WORKFLOW_EXECUTOR_PROMPT (~line 1071), BROWSER_WORKFLOW_FIX_PROMPT (~line 1201), BROWSER_REGRESSION_SWEEP_PROMPT (~line 1248). The "MANDATORY: STEP 0 -- DATA DISCOVERY" block in executor prompt |
| `cli.py` | `src/agent_team/cli.py` | Function signatures: `_run_backend_e2e_tests` (~line 1843), `_run_frontend_e2e_tests` (~line 1885), `_run_browser_workflow_executor` (~line 2031), `_run_browser_regression_sweep` (~line 2161). How prompts are formatted and passed — look for `.format(` calls |

### 1.2 Map the exact injection points

For each of the 6 prompts, identify:
1. The exact line where the prompt's "read these files first" instructions begin
2. The format variables already available (e.g., `{requirements_dir}`, `{app_url}`)
3. Whether `{requirements_dir}` is available (it is for all E2E prompts, it needs to be added for browser prompts)

### 1.3 Map the orchestrator injection point

Find the exact text of:
- Standard workflow step 3: `"3. Deploy RESEARCH FLEET (if needed) → adds research findings"`
- Standard workflow step 3.5: `"3.5. Deploy ARCHITECTURE FLEET → ..."`
- Milestone workflow step a: `"a. Research Fleet → gather knowledge for this milestone's tech"`
- Milestone workflow step b: `"b. Architecture Fleet → design implementation for this milestone"`

The new step goes BETWEEN research and architecture in both workflows.

### 1.4 Verify the format variable availability

In `cli.py`, find where E2E prompts are formatted. Confirm that `requirements_dir` is passed as a format variable. Search for:
```python
BACKEND_E2E_PROMPT.format(
FRONTEND_E2E_PROMPT.format(
E2E_FIX_PROMPT.format(
BROWSER_WORKFLOW_EXECUTOR_PROMPT.format(
```

For browser prompts, check what variables are passed and whether `requirements_dir` can be added.

---

## PHASE 2: IMPLEMENTATION

### 2.1 Edit: RESEARCHER_PROMPT in agents.py (~line 774)

**What to add:** After the existing "## Your Tasks" section (task 4), add task 5:

```
5. Generate TESTING_GUIDANCE.md in the same directory as REQUIREMENTS.md with testing-specific
   findings extracted from your research:

   ## Component Library Testing Patterns
   For each UI component library discovered (e.g., PrimeNG, MUI, Vuetify, Ant Design):
   - DOM structure: how components render (wrapper divs, ARIA roles, class naming conventions)
   - Recommended Playwright selectors: which getByRole/getByText/getByTestId patterns work
   - Known gotchas: virtual scroll, overlay panels, dynamic portal rendering

   ## Canvas / WebGL / Non-DOM Testing Strategy
   If the project uses canvas-based libraries (Fabric.js, Konva, Three.js, D3 with SVG):
   - What CAN be tested via DOM (container elements, toolbar buttons, layer panels)
   - What CANNOT be tested via DOM (objects drawn on canvas — they are pixels, not elements)
   - Alternative testing approaches: programmatic API calls, coordinate-based mouse events,
     canvas.toDataURL() comparison, or testing the toolbar/panel UI around the canvas

   ## Framework Timing Patterns
   For the detected frontend framework:
   - How to wait for async data after navigation (e.g., Angular: waitForResponse, React: waitForSelector)
   - Change detection specifics (Angular Zone.js, React re-render, Vue nextTick)
   - Route transition completion signals

   ## Authentication Testing Specifics
   - Exact login endpoint and request/response shape (from research or REQUIREMENTS.md SVC entries)
   - Token storage mechanism (localStorage, cookie, header)
   - How to programmatically set auth state for faster test setup

   ## Known Library Gotchas
   - List any library-specific testing issues found during research
   - Include version-specific quirks if documented

   If no UI component library, canvas library, or framework-specific findings exist,
   write a minimal TESTING_GUIDANCE.md with just the Authentication section.
   If the project is backend-only, skip TESTING_GUIDANCE.md generation entirely.
```

**Also add to ## Rules section:**
```
- After writing research findings to REQUIREMENTS.md, generate TESTING_GUIDANCE.md
  with testing-specific knowledge extracted from your research (see task 5)
- TESTING_GUIDANCE.md is consumed by downstream testing agents — be SPECIFIC:
  include exact selector patterns, exact wait strategies, exact DOM structures
```

### 2.2 Edit: Orchestrator workflow in agents.py (~line 562)

**Current text (line 562-563):**
```
3. Deploy RESEARCH FLEET (if needed) → adds research findings
   - If design reference URLs are provided, dedicate researcher(s) to design analysis
```

**Replace with:**
```
3. Deploy RESEARCH FLEET (if needed) → adds research findings
   - If design reference URLs are provided, dedicate researcher(s) to design analysis
   - Researchers MUST generate {requirements_dir}/TESTING_GUIDANCE.md with:
     component library selector patterns, canvas/non-DOM testing strategy,
     framework timing patterns, and authentication specifics.
     This file is consumed by E2E and browser testing agents downstream.
```

### 2.3 Edit: Milestone execution workflow in agents.py (~line 351)

**Current text (line 351):**
```
   a. Research Fleet → gather knowledge for this milestone's tech
```

**Replace with:**
```
   a. Research Fleet → gather knowledge for this milestone's tech
      Researchers MUST also generate TESTING_GUIDANCE.md with component library
      selectors, canvas testing strategy, framework timing, and auth specifics.
```

### 2.4 Edit: BACKEND_E2E_PROMPT in e2e_testing.py (~line 556)

**Find the INSTRUCTIONS block (line 562).** After line 562 (`INSTRUCTIONS:`) and before the STEP 0 schema drift check block (line 564), insert:

```
STEP -1 — TESTING GUIDANCE (read BEFORE writing any test):

If {requirements_dir}/TESTING_GUIDANCE.md exists, read it NOW. It contains:
- Component library DOM patterns (how UI components render, which selectors work)
- Canvas/WebGL testing strategy (what can and cannot be tested via DOM)
- Framework-specific timing patterns (how to wait correctly for async operations)
- Authentication flow specifics (exact login sequence and token handling)
- Known library gotchas

Apply ALL guidance from this file when writing test code. For example:
- If it says "PrimeNG DataTable emits role=grid", use page.getByRole('grid')
- If it says "Angular needs waitForResponse after navigation", use that pattern
- If it says "canvas objects are not DOM elements", test the toolbar/panel UI instead

If TESTING_GUIDANCE.md does not exist, proceed with generic Playwright best practices.

```

### 2.5 Edit: FRONTEND_E2E_PROMPT in e2e_testing.py (~line 732)

**Find the INSTRUCTIONS block (line 744).** After `INSTRUCTIONS:` (line 744) and before `1. Install Playwright` (line 745), insert the same STEP -1 block:

```
STEP -1 — TESTING GUIDANCE (read BEFORE writing any test):

If {requirements_dir}/TESTING_GUIDANCE.md exists, read it NOW. It contains:
- Component library DOM patterns (how UI components render, which selectors work)
- Canvas/WebGL testing strategy (what can and cannot be tested via DOM)
- Framework-specific timing patterns (how to wait correctly for async operations)
- Authentication flow specifics (exact login sequence and token handling)
- Known library gotchas

Apply ALL guidance from this file when writing Playwright test code. For example:
- If it says "PrimeNG DataTable emits role=grid", use page.getByRole('grid') not getByTestId
- If it says "Angular needs waitForResponse after navigation", use that instead of waitForTimeout
- If it says "Fabric.js canvas objects are not DOM elements", test toolbar/panel UI instead
- If it says "dropdown renders in portal outside component", use page.locator('.p-overlay')

If TESTING_GUIDANCE.md does not exist, proceed with generic Playwright best practices.

```

### 2.6 Edit: E2E_FIX_PROMPT in e2e_testing.py (~line 884)

**Find the INSTRUCTIONS block (line 893).** After line 893 (`INSTRUCTIONS:`) and before `0. FIRST: Read` (line 894), insert:

```
TESTING GUIDANCE: If {requirements_dir}/TESTING_GUIDANCE.md exists, read it BEFORE
analyzing failures. It may explain WHY a test fails (e.g., wrong selector for a
component library, impossible canvas interaction, missing async wait). A fix that
ignores testing guidance will likely fail again.

```

### 2.7 Edit: BROWSER_WORKFLOW_EXECUTOR_PROMPT in browser_testing.py (~line 1071)

**Find the "## MANDATORY: STEP 0 -- DATA DISCOVERY" block (~line 1088).** BEFORE that block, insert a new section:

```
## STEP -1: TESTING GUIDANCE (read BEFORE any browser interaction)

Search for TESTING_GUIDANCE.md in the project's .agent-team/ directory (or requirements directory).
If found, read it COMPLETELY. It contains domain-specific testing knowledge from the research phase:

- **Component library selectors**: How the app's UI components actually render in the DOM.
  Use the EXACT selectors documented here instead of guessing. For example, if it says
  "PrimeNG DataTable renders with role=grid", look for that role in browser_snapshot() output.

- **Canvas/WebGL strategy**: If the app has canvas-based features (drawing, charts, maps),
  the guidance explains what you CAN and CANNOT interact with via browser tools.
  If it says "canvas objects are not DOM elements — test the toolbar buttons around the canvas
  instead", do exactly that. Do NOT waste steps trying to click inside a <canvas>.

- **Framework timing**: How long to wait after navigation, form submission, or data mutation.
  Use the specific wait strategy documented (e.g., waitForResponse, waitForSelector) instead
  of arbitrary timeouts.

- **Auth flow**: Exact login sequence, credential format, token storage mechanism.

If TESTING_GUIDANCE.md does not exist, proceed with standard browser testing approach.

```

### 2.8 Edit: BROWSER_WORKFLOW_FIX_PROMPT in browser_testing.py (~line 1201)

**Find the "## Instructions" block (~line 1217).** After `## Instructions` and before `1. **ANALYZE**` (line 1219), insert:

```
0. **READ TESTING GUIDANCE**: If .agent-team/TESTING_GUIDANCE.md exists, read it FIRST.
   It may explain the root cause of the failure (wrong selector for component library,
   impossible canvas interaction, missing async wait pattern). Apply its recommendations
   before attempting any fix.

```

### 2.9 Edit: BROWSER_REGRESSION_SWEEP_PROMPT in browser_testing.py (~line 1248)

**Find the "## Instructions" block (~line 1262).** After `## Instructions` and before `For EACH URL` (line 1264), insert:

```
If .agent-team/TESTING_GUIDANCE.md exists, read it first. It contains component library
DOM patterns that help you verify pages loaded correctly (e.g., PrimeNG DataTable should
show role="grid", not just an empty div).

```

### 2.10 Summary of all edits

| File | Location | Edit Type | Size |
|------|----------|-----------|------|
| `agents.py` | RESEARCHER_PROMPT (~line 774) | Add task 5 + rules | ~30 lines |
| `agents.py` | Orchestrator step 3 (~line 562) | Extend existing text | ~3 lines |
| `agents.py` | Milestone step a (~line 351) | Extend existing text | ~2 lines |
| `e2e_testing.py` | BACKEND_E2E_PROMPT (~line 562) | Insert STEP -1 block | ~15 lines |
| `e2e_testing.py` | FRONTEND_E2E_PROMPT (~line 744) | Insert STEP -1 block | ~16 lines |
| `e2e_testing.py` | E2E_FIX_PROMPT (~line 893) | Insert guidance note | ~4 lines |
| `browser_testing.py` | BROWSER_WORKFLOW_EXECUTOR_PROMPT (~line 1088) | Insert STEP -1 block | ~20 lines |
| `browser_testing.py` | BROWSER_WORKFLOW_FIX_PROMPT (~line 1217) | Insert step 0 | ~4 lines |
| `browser_testing.py` | BROWSER_REGRESSION_SWEEP_PROMPT (~line 1262) | Insert note | ~3 lines |
| **Total** | | | **~97 lines of prompt text** |

---

## PHASE 3: TESTS

Create `tests/test_v13_research_testing_pipeline.py`.

### Test Class 1: TestResearcherPromptHasTestingGuidance (~4 tests)

```python
class TestResearcherPromptHasTestingGuidance:
    """Verify RESEARCHER_PROMPT instructs researchers to generate TESTING_GUIDANCE.md."""

    def test_researcher_prompt_mentions_testing_guidance(self):
        """RESEARCHER_PROMPT contains 'TESTING_GUIDANCE.md'."""
        from agent_team.agents import RESEARCHER_PROMPT
        assert "TESTING_GUIDANCE.md" in RESEARCHER_PROMPT

    def test_researcher_prompt_has_component_library_section(self):
        """RESEARCHER_PROMPT instructs component library testing patterns."""
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Component Library Testing Patterns" in RESEARCHER_PROMPT

    def test_researcher_prompt_has_canvas_section(self):
        """RESEARCHER_PROMPT instructs canvas/non-DOM testing strategy."""
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Canvas" in RESEARCHER_PROMPT or "canvas" in RESEARCHER_PROMPT

    def test_researcher_prompt_has_timing_section(self):
        """RESEARCHER_PROMPT instructs framework timing patterns."""
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Timing Patterns" in RESEARCHER_PROMPT or "timing" in RESEARCHER_PROMPT.lower()
```

### Test Class 2: TestOrchestratorWorkflowMentionsTestingGuidance (~3 tests)

```python
class TestOrchestratorWorkflowMentionsTestingGuidance:
    """Verify orchestrator workflow steps reference TESTING_GUIDANCE.md."""

    def test_orchestrator_prompt_mentions_testing_guidance(self):
        """build_orchestrator_prompt output contains TESTING_GUIDANCE.md reference."""
        from agent_team.agents import build_orchestrator_prompt
        from agent_team.config import AgentTeamConfig
        prompt = build_orchestrator_prompt(
            task="Build a web app",
            depth="standard",
            config=AgentTeamConfig(),
        )
        assert "TESTING_GUIDANCE.md" in prompt

    def test_milestone_prompt_mentions_testing_guidance(self):
        """build_milestone_execution_prompt output contains TESTING_GUIDANCE.md reference."""
        from agent_team.agents import build_milestone_execution_prompt
        from agent_team.config import AgentTeamConfig
        prompt = build_milestone_execution_prompt(
            task="Build a web app",
            depth="standard",
            config=AgentTeamConfig(),
        )
        assert "TESTING_GUIDANCE.md" in prompt

    def test_orchestrator_research_step_mentions_guidance(self):
        """The research fleet step explicitly mentions TESTING_GUIDANCE.md generation."""
        from agent_team.agents import build_orchestrator_prompt
        from agent_team.config import AgentTeamConfig
        prompt = build_orchestrator_prompt(
            task="Build a web app",
            depth="standard",
            config=AgentTeamConfig(),
        )
        # Check that the research fleet step (step 3) mentions it
        research_idx = prompt.find("Deploy RESEARCH FLEET")
        arch_idx = prompt.find("Deploy ARCHITECTURE FLEET")
        assert research_idx != -1
        assert arch_idx != -1
        research_block = prompt[research_idx:arch_idx]
        assert "TESTING_GUIDANCE.md" in research_block
```

### Test Class 3: TestE2EPromptsInjectTestingGuidance (~6 tests)

```python
class TestE2EPromptsInjectTestingGuidance:
    """Verify E2E testing prompts instruct agents to read TESTING_GUIDANCE.md."""

    def test_backend_e2e_prompt_has_testing_guidance(self):
        """BACKEND_E2E_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.e2e_testing import BACKEND_E2E_PROMPT
        assert "TESTING_GUIDANCE.md" in BACKEND_E2E_PROMPT

    def test_frontend_e2e_prompt_has_testing_guidance(self):
        """FRONTEND_E2E_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.e2e_testing import FRONTEND_E2E_PROMPT
        assert "TESTING_GUIDANCE.md" in FRONTEND_E2E_PROMPT

    def test_e2e_fix_prompt_has_testing_guidance(self):
        """E2E_FIX_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.e2e_testing import E2E_FIX_PROMPT
        assert "TESTING_GUIDANCE.md" in E2E_FIX_PROMPT

    def test_backend_prompt_step_minus_1_before_step_0(self):
        """STEP -1 (testing guidance) appears before STEP 0 (schema drift)."""
        from agent_team.e2e_testing import BACKEND_E2E_PROMPT
        step_neg1 = BACKEND_E2E_PROMPT.find("STEP -1")
        step_0 = BACKEND_E2E_PROMPT.find("STEP 0")
        assert step_neg1 != -1, "STEP -1 not found"
        assert step_0 != -1, "STEP 0 not found"
        assert step_neg1 < step_0, "STEP -1 must come before STEP 0"

    def test_frontend_prompt_step_minus_1_before_install(self):
        """STEP -1 (testing guidance) appears before Playwright install instruction."""
        from agent_team.e2e_testing import FRONTEND_E2E_PROMPT
        step_neg1 = FRONTEND_E2E_PROMPT.find("STEP -1")
        install = FRONTEND_E2E_PROMPT.find("npx playwright install")
        assert step_neg1 != -1, "STEP -1 not found"
        assert install != -1, "Playwright install not found"
        assert step_neg1 < install, "STEP -1 must come before install"

    def test_e2e_prompts_use_requirements_dir_variable(self):
        """All E2E prompts reference TESTING_GUIDANCE.md via {requirements_dir} variable."""
        from agent_team.e2e_testing import BACKEND_E2E_PROMPT, FRONTEND_E2E_PROMPT, E2E_FIX_PROMPT
        for name, prompt in [
            ("BACKEND", BACKEND_E2E_PROMPT),
            ("FRONTEND", FRONTEND_E2E_PROMPT),
            ("FIX", E2E_FIX_PROMPT),
        ]:
            assert "{requirements_dir}/TESTING_GUIDANCE.md" in prompt, (
                f"{name} prompt should use {{requirements_dir}}/TESTING_GUIDANCE.md"
            )
```

### Test Class 4: TestBrowserPromptsInjectTestingGuidance (~5 tests)

```python
class TestBrowserPromptsInjectTestingGuidance:
    """Verify browser testing prompts instruct agents to read TESTING_GUIDANCE.md."""

    def test_executor_prompt_has_testing_guidance(self):
        """BROWSER_WORKFLOW_EXECUTOR_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.browser_testing import BROWSER_WORKFLOW_EXECUTOR_PROMPT
        assert "TESTING_GUIDANCE.md" in BROWSER_WORKFLOW_EXECUTOR_PROMPT

    def test_fix_prompt_has_testing_guidance(self):
        """BROWSER_WORKFLOW_FIX_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.browser_testing import BROWSER_WORKFLOW_FIX_PROMPT
        assert "TESTING_GUIDANCE.md" in BROWSER_WORKFLOW_FIX_PROMPT

    def test_regression_prompt_has_testing_guidance(self):
        """BROWSER_REGRESSION_SWEEP_PROMPT contains TESTING_GUIDANCE.md reference."""
        from agent_team.browser_testing import BROWSER_REGRESSION_SWEEP_PROMPT
        assert "TESTING_GUIDANCE.md" in BROWSER_REGRESSION_SWEEP_PROMPT

    def test_executor_step_minus_1_before_step_0(self):
        """STEP -1 (testing guidance) appears before STEP 0 (data discovery) in executor."""
        from agent_team.browser_testing import BROWSER_WORKFLOW_EXECUTOR_PROMPT
        step_neg1 = BROWSER_WORKFLOW_EXECUTOR_PROMPT.find("STEP -1")
        step_0 = BROWSER_WORKFLOW_EXECUTOR_PROMPT.find("STEP 0")
        assert step_neg1 != -1, "STEP -1 not found"
        assert step_0 != -1, "STEP 0 not found"
        assert step_neg1 < step_0, "STEP -1 must come before STEP 0"

    def test_executor_mentions_canvas_strategy(self):
        """Executor prompt mentions canvas/non-DOM testing strategy."""
        from agent_team.browser_testing import BROWSER_WORKFLOW_EXECUTOR_PROMPT
        prompt_lower = BROWSER_WORKFLOW_EXECUTOR_PROMPT.lower()
        assert "canvas" in prompt_lower, "Executor prompt should mention canvas"
```

### Test Class 5: TestTestingGuidanceContentRequirements (~5 tests)

```python
class TestTestingGuidanceContentRequirements:
    """Verify the RESEARCHER_PROMPT specifies all required TESTING_GUIDANCE.md sections."""

    def test_requires_component_library_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Component Library Testing Patterns" in RESEARCHER_PROMPT

    def test_requires_canvas_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Canvas" in RESEARCHER_PROMPT and "Non-DOM" in RESEARCHER_PROMPT

    def test_requires_framework_timing_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Framework Timing Patterns" in RESEARCHER_PROMPT

    def test_requires_authentication_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Authentication Testing Specifics" in RESEARCHER_PROMPT

    def test_requires_known_gotchas_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Known" in RESEARCHER_PROMPT and "Gotcha" in RESEARCHER_PROMPT
```

### Test Class 6: TestNoRegressionOnExistingPromptContent (~6 tests)

```python
class TestNoRegressionOnExistingPromptContent:
    """Verify existing prompt content is preserved — edits are ADDITIVE only."""

    def test_backend_e2e_still_has_schema_drift_check(self):
        from agent_team.e2e_testing import BACKEND_E2E_PROMPT
        assert "SCHEMA DRIFT CHECK" in BACKEND_E2E_PROMPT

    def test_backend_e2e_still_has_role_based_testing(self):
        from agent_team.e2e_testing import BACKEND_E2E_PROMPT
        assert "ROLE-BASED API TESTING" in BACKEND_E2E_PROMPT

    def test_frontend_e2e_still_has_prd_feature_coverage(self):
        from agent_team.e2e_testing import FRONTEND_E2E_PROMPT
        assert "PRD FEATURE COVERAGE" in FRONTEND_E2E_PROMPT

    def test_executor_still_has_anti_cheat_rules(self):
        from agent_team.browser_testing import BROWSER_WORKFLOW_EXECUTOR_PROMPT
        assert "ANTI-CHEAT RULES" in BROWSER_WORKFLOW_EXECUTOR_PROMPT

    def test_executor_still_has_deep_verification(self):
        from agent_team.browser_testing import BROWSER_WORKFLOW_EXECUTOR_PROMPT
        assert "DEEP VERIFICATION RULES" in BROWSER_WORKFLOW_EXECUTOR_PROMPT

    def test_researcher_still_has_design_reference_section(self):
        from agent_team.agents import RESEARCHER_PROMPT
        assert "Design Reference Research" in RESEARCHER_PROMPT
```

**Total: ~29 tests across 6 classes.**

---

## PHASE 4: WIRING VERIFICATION

After implementation, verify all connections:

### 4.1 Prompt Content Verification

- [ ] `RESEARCHER_PROMPT` contains "TESTING_GUIDANCE.md" (task 5 + rules)
- [ ] `RESEARCHER_PROMPT` specifies all 5 sections (Component Library, Canvas, Timing, Auth, Gotchas)
- [ ] `RESEARCHER_PROMPT` says "skip TESTING_GUIDANCE.md if backend-only"
- [ ] Orchestrator step 3 text mentions TESTING_GUIDANCE.md generation
- [ ] Milestone step a text mentions TESTING_GUIDANCE.md generation

### 4.2 E2E Prompt Injection Verification

- [ ] `BACKEND_E2E_PROMPT` has STEP -1 block with `{requirements_dir}/TESTING_GUIDANCE.md`
- [ ] `BACKEND_E2E_PROMPT` STEP -1 appears BEFORE STEP 0 (schema drift)
- [ ] `FRONTEND_E2E_PROMPT` has STEP -1 block with `{requirements_dir}/TESTING_GUIDANCE.md`
- [ ] `FRONTEND_E2E_PROMPT` STEP -1 appears BEFORE `npx playwright install`
- [ ] `E2E_FIX_PROMPT` has guidance note with `{requirements_dir}/TESTING_GUIDANCE.md`
- [ ] All 3 E2E prompts use `{requirements_dir}` variable (NOT hardcoded path)

### 4.3 Browser Prompt Injection Verification

- [ ] `BROWSER_WORKFLOW_EXECUTOR_PROMPT` has STEP -1 block before STEP 0
- [ ] `BROWSER_WORKFLOW_EXECUTOR_PROMPT` mentions canvas, component library, timing
- [ ] `BROWSER_WORKFLOW_FIX_PROMPT` has step 0 guidance note
- [ ] `BROWSER_REGRESSION_SWEEP_PROMPT` has guidance note
- [ ] Browser prompts reference `.agent-team/TESTING_GUIDANCE.md` (or equivalent path)

### 4.4 No-Regression Verification

- [ ] `BACKEND_E2E_PROMPT` still contains: SCHEMA DRIFT CHECK, ROLE-BASED API TESTING, Mutation Verification Rule, Endpoint Exhaustiveness Rule
- [ ] `FRONTEND_E2E_PROMPT` still contains: PRD FEATURE COVERAGE, PLACEHOLDER DETECTION, DEAD COMPONENT DETECTION, INTERACTION DEPTH, FORM SUBMISSION VERIFICATION
- [ ] `E2E_FIX_PROMPT` still contains: PATTERN-SPECIFIC FIX GUIDANCE, TEST CORRECTION EXCEPTION, GUARD RAIL
- [ ] `BROWSER_WORKFLOW_EXECUTOR_PROMPT` still contains: ANTI-CHEAT RULES, DEEP VERIFICATION RULES, DATA DISCOVERY
- [ ] `RESEARCHER_PROMPT` still contains: Design Reference Research, Context7, Firecrawl instructions

### 4.5 Format Variable Compatibility

- [ ] `{requirements_dir}` is passed when formatting `BACKEND_E2E_PROMPT` (verify in cli.py)
- [ ] `{requirements_dir}` is passed when formatting `FRONTEND_E2E_PROMPT` (verify in cli.py)
- [ ] `{requirements_dir}` is passed when formatting `E2E_FIX_PROMPT` (verify in cli.py)
- [ ] Browser prompts' TESTING_GUIDANCE.md path resolves correctly at runtime

---

## PHASE 5: RUN TESTS

```bash
# Run only v13 tests
pytest tests/test_v13_research_testing_pipeline.py -v

# Run full suite to check for regressions
pytest tests/ -v --tb=short 2>&1 | tail -20

# Verify test count
pytest tests/test_v13_research_testing_pipeline.py --collect-only -q | tail -1
# Expected: ~29 tests collected
```

**Pass criteria:**
- All 29 v13 tests pass
- Zero new failures in existing test suite
- Total passing count >= previous total (check MEMORY.md — last was 5217)

---

## PHASE 6: FINAL REPORT

Write `V13_RESEARCH_TESTING_PIPELINE_REPORT.md` in the project root with:

1. **Changes summary** — each file edited, lines changed, what was added
2. **Test results** — pass count, any failures, regression check
3. **The three failure scenarios** — how each is now prevented:
   - PrimeNG selector: researcher documents DOM structure → testing agent reads it → correct selector
   - Canvas interaction: researcher documents impossibility → testing agent skips canvas clicks → tests toolbar instead
   - Angular timing: researcher documents waitForResponse pattern → testing agent uses it → no flakes
4. **What this does NOT do** — this is a prompt-only change. If the research fleet doesn't use Context7 to look up PrimeNG docs, no TESTING_GUIDANCE.md sections will be populated. The fix ensures the pipeline EXISTS — the research fleet's actual thoroughness depends on the researcher agent's execution quality.

---

## WHAT NOT TO DO

| Anti-Pattern | Why |
|-------------|-----|
| Create a new Python module for this | Unnecessary — this is purely prompt text edits to 3 existing files |
| Add config fields for TESTING_GUIDANCE.md | Unnecessary — the file is always generated when researchers run. No config gate needed |
| Modify cli.py to check if TESTING_GUIDANCE.md exists | Unnecessary — the prompts say "if exists, read it." No CLI validation needed |
| Add a dedicated "testing guidance generator" agent | Over-engineering — the researcher already does research, just tell it to write one more file |
| Inject TESTING_GUIDANCE.md content into the prompt string | Wrong approach — the file may be large. Tell agents to READ it, don't inline it |
| Add `{testing_guidance_path}` as a new format variable | Unnecessary for E2E prompts (already have `{requirements_dir}`). For browser prompts, use `.agent-team/` directly since the executor already searches the project |
| Remove or modify existing prompt content | ALL edits are ADDITIVE. Never delete existing instructions |

---

## SIZE TARGETS

| Metric | Target |
|--------|--------|
| Lines added to `agents.py` | ~35 (RESEARCHER_PROMPT + orchestrator + milestone) |
| Lines added to `e2e_testing.py` | ~35 (3 prompt constants) |
| Lines added to `browser_testing.py` | ~27 (3 prompt constants) |
| Lines in `test_v13_research_testing_pipeline.py` | ~150 (29 tests across 6 classes) |
| Total new code | ~247 lines |
| Files modified | 3 (agents.py, e2e_testing.py, browser_testing.py) |
| Files created | 1 (test_v13_research_testing_pipeline.py) |
| New config fields | 0 |
| New Python functions | 0 |
| New dataclasses | 0 |

This is a **prompt-only** feature. Zero new runtime code. Zero new config. Zero new dependencies. The entire implementation is injecting the right words into the right prompts so that existing agents generate and consume one new markdown file.
